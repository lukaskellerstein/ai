"quantization" = decrease from 32bit to 16bit, 8bit, 4 bit

- 8 bit (vs. 16bit)
- need 1/2 less computing power on GPU
- is slower than 16bit
- does not degrade predictive performance of large models
